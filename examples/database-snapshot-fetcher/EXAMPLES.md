# ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªæ–‡æ¡£åŒ…å«äº†æ•°æ®åº“å¿«ç…§è·å–ç³»ç»Ÿçš„å„ç§ä½¿ç”¨ç¤ºä¾‹ã€‚

## ğŸ“‹ ç›®å½•

1. [åŸºç¡€ç¤ºä¾‹](#åŸºç¡€ç¤ºä¾‹)
2. [å®šæ—¶ä»»åŠ¡ç¤ºä¾‹](#å®šæ—¶ä»»åŠ¡ç¤ºä¾‹)
3. [é«˜çº§æŸ¥è¯¢ç¤ºä¾‹](#é«˜çº§æŸ¥è¯¢ç¤ºä¾‹)
4. [é”™è¯¯å¤„ç†ç¤ºä¾‹](#é”™è¯¯å¤„ç†ç¤ºä¾‹)
5. [é›†æˆç¤ºä¾‹](#é›†æˆç¤ºä¾‹)

---

## åŸºç¡€ç¤ºä¾‹

### ç¤ºä¾‹1: æ‰‹åŠ¨è§¦å‘è·å–æœ€è¿‘1å°æ—¶çš„æ•°æ®

```bash
curl -X POST http://localhost:8080/database-snapshot-manual \
  -H "Content-Type: application/json" \
  -H "Accept: application/json" \
  -d '{
    "tagNames": ["æ¸©åº¦ä¼ æ„Ÿå™¨01", "å‹åŠ›ä¼ æ„Ÿå™¨01", "æµé‡è®¡01"],
    "count": 3600,
    "startTime": "2024-11-01 11:00:00.000",
    "endTime": "2024-11-01 12:00:00.000"
  }'
```

**å“åº”:**
```json
{
  "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  "workflowdata": {
    "status": "success",
    "data": [
      {
        "tagName": "æ¸©åº¦ä¼ æ„Ÿå™¨01",
        "values": [...]
      }
    ],
    "requestId": "req-20241101-001",
    "fetchTime": "2024-11-01T12:00:05Z"
  }
}
```

### ç¤ºä¾‹2: è·å–å¤šä¸ªæ ‡ç­¾ç‚¹è¿‡å»24å°æ—¶çš„æ•°æ®

```bash
# ä½¿ç”¨å˜é‡ç®€åŒ–å‘½ä»¤
END_TIME=$(date "+%Y-%m-%d %H:00:00.000")
START_TIME=$(date -d "24 hours ago" "+%Y-%m-%d %H:00:00.000")

curl -X POST http://localhost:8080/database-snapshot-manual \
  -H "Content-Type: application/json" \
  -d "{
    \"tagNames\": [\"Tag001\", \"Tag002\", \"Tag003\", \"Tag004\", \"Tag005\"],
    \"count\": 1440,
    \"startTime\": \"$START_TIME\",
    \"endTime\": \"$END_TIME\"
  }"
```

### ç¤ºä¾‹3: æŸ¥è¯¢å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€

```bash
# ä¿å­˜å·¥ä½œæµID
WORKFLOW_ID="f47ac10b-58cc-4372-a567-0e02b2c3d479"

# æŸ¥è¯¢çŠ¶æ€
curl http://localhost:8080/database-snapshot-manual/$WORKFLOW_ID | jq '.'
```

**å“åº”:**
```json
{
  "id": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
  "workflowdata": {
    "status": "success",
    "data": [...],
    "requestId": "req-20241101-001",
    "fetchTime": "2024-11-01T12:00:05Z"
  }
}
```

---

## å®šæ—¶ä»»åŠ¡ç¤ºä¾‹

### ç¤ºä¾‹4: æ¯15åˆ†é’Ÿè·å–ä¸€æ¬¡æ•°æ®

ç¼–è¾‘ `src/main/resources/database-snapshot-workflow.sw.json`:

```json
{
  "id": "database-snapshot-workflow",
  "start": {
    "stateName": "FetchDatabaseSnapshot",
    "schedule": {
      "cron": "0 */15 * * * ?"
    }
  },
  "functions": [...],
  "states": [...]
}
```

### ç¤ºä¾‹5: æ¯å¤©å‡Œæ™¨2ç‚¹è·å–å‰ä¸€å¤©çš„æ•°æ®

```json
{
  "start": {
    "schedule": {
      "cron": "0 0 2 * * ?"
    }
  }
}
```

åœ¨workflowçš„åˆå§‹stateä¸­åŠ¨æ€è®¡ç®—æ—¶é—´ï¼š

```json
{
  "name": "PrepareTimeRange",
  "type": "inject",
  "data": {
    "endTime": "${ now() | strftime(\"%Y-%m-%d 00:00:00.000\") }",
    "startTime": "${ (now() - 86400) | strftime(\"%Y-%m-%d 00:00:00.000\") }"
  },
  "transition": "FetchDatabaseSnapshot"
}
```

### ç¤ºä¾‹6: å·¥ä½œæ—¥æ¯å°æ—¶æ‰§è¡Œ

```json
{
  "schedule": {
    "cron": "0 0 * * * MON-FRI"
  }
}
```

---

## é«˜çº§æŸ¥è¯¢ç¤ºä¾‹

### ç¤ºä¾‹7: æ‰¹é‡æŸ¥è¯¢å¤šç»„æ ‡ç­¾ç‚¹

```bash
# ç¬¬ä¸€ç»„ï¼šæ¸©åº¦ä¼ æ„Ÿå™¨
curl -X POST http://localhost:8080/database-snapshot-manual \
  -H "Content-Type: application/json" \
  -d '{
    "tagNames": ["æ¸©åº¦01", "æ¸©åº¦02", "æ¸©åº¦03", "æ¸©åº¦04", "æ¸©åº¦05"],
    "count": 100,
    "startTime": "2024-11-01 00:00:00.000",
    "endTime": "2024-11-01 12:00:00.000"
  }'

# ç¬¬äºŒç»„ï¼šå‹åŠ›ä¼ æ„Ÿå™¨
curl -X POST http://localhost:8080/database-snapshot-manual \
  -H "Content-Type: application/json" \
  -d '{
    "tagNames": ["å‹åŠ›01", "å‹åŠ›02", "å‹åŠ›03", "å‹åŠ›04", "å‹åŠ›05"],
    "count": 100,
    "startTime": "2024-11-01 00:00:00.000",
    "endTime": "2024-11-01 12:00:00.000"
  }'
```

### ç¤ºä¾‹8: Pythonè„šæœ¬è‡ªåŠ¨åŒ–æ‰¹é‡æŸ¥è¯¢

åˆ›å»ºæ–‡ä»¶ `batch_query.py`:

```python
#!/usr/bin/env python3
import requests
import json
from datetime import datetime, timedelta

# é…ç½®
BASE_URL = "http://localhost:8080"
ENDPOINT = f"{BASE_URL}/database-snapshot-manual"

# æ—¶é—´èŒƒå›´
end_time = datetime.now()
start_time = end_time - timedelta(hours=1)

# æ ‡ç­¾ç‚¹åˆ†ç»„
tag_groups = {
    "æ¸©åº¦ä¼ æ„Ÿå™¨": ["æ¸©åº¦01", "æ¸©åº¦02", "æ¸©åº¦03"],
    "å‹åŠ›ä¼ æ„Ÿå™¨": ["å‹åŠ›01", "å‹åŠ›02", "å‹åŠ›03"],
    "æµé‡è®¡": ["æµé‡01", "æµé‡02", "æµé‡03"]
}

# æ‰¹é‡æŸ¥è¯¢
results = {}
for group_name, tags in tag_groups.items():
    payload = {
        "tagNames": tags,
        "count": 100,
        "startTime": start_time.strftime("%Y-%m-%d %H:%M:%S.000"),
        "endTime": end_time.strftime("%Y-%m-%d %H:%M:%S.000")
    }

    response = requests.post(ENDPOINT, json=payload)
    if response.status_code == 201:
        results[group_name] = response.json()
        print(f"âœ“ {group_name}: å·¥ä½œæµID = {results[group_name]['id']}")
    else:
        print(f"âœ— {group_name}: å¤±è´¥ - {response.status_code}")

# ä¿å­˜ç»“æœ
with open("query_results.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\næ€»è®¡æŸ¥è¯¢ {len(results)} ç»„æ ‡ç­¾ç‚¹")
```

è¿è¡Œï¼š
```bash
chmod +x batch_query.py
./batch_query.py
```

### ç¤ºä¾‹9: ä½¿ç”¨jqå¤„ç†å“åº”æ•°æ®

```bash
# æå–æ‰€æœ‰æˆåŠŸçš„æ•°æ®
curl -s http://localhost:8080/database-snapshot-manual/$WORKFLOW_ID | \
  jq '.workflowdata | select(.status == "success") | .data'

# ç»Ÿè®¡æ•°æ®ç‚¹æ•°é‡
curl -s http://localhost:8080/database-snapshot-manual/$WORKFLOW_ID | \
  jq '.workflowdata.data | length'

# æå–RequestID
curl -s http://localhost:8080/database-snapshot-manual/$WORKFLOW_ID | \
  jq -r '.workflowdata.requestId'
```

---

## é”™è¯¯å¤„ç†ç¤ºä¾‹

### ç¤ºä¾‹10: å¤„ç†APIè¶…æ—¶

ä¿®æ”¹ `application.properties`:

```properties
# å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
quarkus.rest-client."database-history-api".read-timeout=60000
quarkus.rest-client."database-history-api".connect-timeout=15000
```

### ç¤ºä¾‹11: é‡è¯•æœºåˆ¶

åœ¨workflowä¸­æ·»åŠ é‡è¯•é€»è¾‘ï¼ˆä¿®æ”¹ `database-snapshot-manual.sw.json`ï¼‰:

```json
{
  "name": "FetchDatabaseSnapshot",
  "type": "operation",
  "actions": [
    {
      "name": "è·å–æ•°æ®åº“å†å²å¿«ç…§",
      "functionRef": {
        "refName": "getArchivedValuesBatch",
        "arguments": {...}
      }
    }
  ],
  "onErrors": [
    {
      "errorRef": "TimeoutError",
      "transition": "RetryFetch"
    }
  ]
},
{
  "name": "RetryFetch",
  "type": "sleep",
  "duration": "PT5S",
  "transition": "FetchDatabaseSnapshot"
}
```

### ç¤ºä¾‹12: é”™è¯¯é€šçŸ¥

æ·»åŠ é”™è¯¯é€šçŸ¥state:

```json
{
  "name": "HandleError",
  "type": "operation",
  "actions": [
    {
      "name": "å‘é€é”™è¯¯é€šçŸ¥",
      "functionRef": {
        "refName": "sendNotification",
        "arguments": {
          "level": "error",
          "message": "\"æ•°æ®è·å–å¤±è´¥: \" + .snapshotData.Error",
          "timestamp": "${ now() }"
        }
      }
    }
  ],
  "end": true
}
```

---

## é›†æˆç¤ºä¾‹

### ç¤ºä¾‹13: ä¸Prometheusé›†æˆç›‘æ§

è®¿é—®æŒ‡æ ‡ç«¯ç‚¹ï¼š
```bash
curl http://localhost:8080/q/metrics
```

æ·»åŠ åˆ° `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'database-snapshot-fetcher'
    metrics_path: '/q/metrics'
    static_configs:
      - targets: ['localhost:8080']
```

### ç¤ºä¾‹14: é€šè¿‡Webhookè§¦å‘

åˆ›å»ºä¸€ä¸ªç®€å•çš„webhookæœåŠ¡ï¼š

```python
#!/usr/bin/env python3
from flask import Flask, request
import requests

app = Flask(__name__)
WORKFLOW_URL = "http://localhost:8080/database-snapshot-manual"

@app.route('/webhook/trigger', methods=['POST'])
def trigger_workflow():
    # ä»webhookè·å–å‚æ•°
    data = request.json

    # æ„é€ workflowè¯·æ±‚
    payload = {
        "tagNames": data.get("tags", ["Tag001"]),
        "count": data.get("count", 100),
        "startTime": data.get("startTime"),
        "endTime": data.get("endTime")
    }

    # è§¦å‘workflow
    response = requests.post(WORKFLOW_URL, json=payload)
    return response.json()

if __name__ == '__main__':
    app.run(port=5001)
```

è§¦å‘ï¼š
```bash
curl -X POST http://localhost:5001/webhook/trigger \
  -H "Content-Type: application/json" \
  -d '{
    "tags": ["Tag001", "Tag002"],
    "count": 100,
    "startTime": "2024-11-01 00:00:00.000",
    "endTime": "2024-11-01 12:00:00.000"
  }'
```

### ç¤ºä¾‹15: æ•°æ®å¯¼å‡ºåˆ°CSV

åˆ›å»º `export_to_csv.sh`:

```bash
#!/bin/bash

WORKFLOW_ID=$1
OUTPUT_FILE="snapshot_data_$(date +%Y%m%d_%H%M%S).csv"

# è·å–æ•°æ®
DATA=$(curl -s http://localhost:8080/database-snapshot-manual/$WORKFLOW_ID)

# ä½¿ç”¨jqè½¬æ¢ä¸ºCSV
echo "$DATA" | jq -r '
  .workflowdata.data[] |
  [.tagName, .timestamp, .value, .quality] |
  @csv
' > $OUTPUT_FILE

echo "æ•°æ®å·²å¯¼å‡ºåˆ°: $OUTPUT_FILE"
```

ä½¿ç”¨ï¼š
```bash
./export_to_csv.sh f47ac10b-58cc-4372-a567-0e02b2c3d479
```

### ç¤ºä¾‹16: ä¸æ•°æ®åº“é›†æˆä¿å­˜ç»“æœ

æ·»åŠ PostgreSQLä¾èµ–åˆ° `pom.xml`:

```xml
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-jdbc-postgresql</artifactId>
</dependency>
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-hibernate-orm-panache</artifactId>
</dependency>
```

é…ç½®æ•°æ®åº“ `application.properties`:

```properties
quarkus.datasource.db-kind=postgresql
quarkus.datasource.username=postgres
quarkus.datasource.password=postgres
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/snapshot_db

quarkus.hibernate-orm.database.generation=update
```

åˆ›å»ºEntity:

```java
@Entity
@Table(name = "snapshot_records")
public class SnapshotRecord extends PanacheEntity {

    @Column(name = "workflow_id")
    public String workflowId;

    @Column(name = "request_id")
    public String requestId;

    @Column(name = "status")
    public String status;

    @Column(name = "fetch_time")
    public Instant fetchTime;

    @Column(name = "data", columnDefinition = "jsonb")
    public String data;
}
```

---

## ğŸ”§ Shellè„šæœ¬å·¥å…·é›†

### å®Œæ•´çš„ç®¡ç†è„šæœ¬

åˆ›å»º `manage.sh`:

```bash
#!/bin/bash

function show_menu() {
    echo "================================"
    echo "æ•°æ®åº“å¿«ç…§è·å–ç³»ç»Ÿ - ç®¡ç†å·¥å…·"
    echo "================================"
    echo "1. å¯åŠ¨æœåŠ¡"
    echo "2. åœæ­¢æœåŠ¡"
    echo "3. æŸ¥çœ‹æ—¥å¿—"
    echo "4. è§¦å‘æ‰‹åŠ¨è·å–"
    echo "5. æŸ¥çœ‹æœ€è¿‘çš„å·¥ä½œæµ"
    echo "6. å¥åº·æ£€æŸ¥"
    echo "7. æŸ¥çœ‹Swagger UI"
    echo "0. é€€å‡º"
    echo "================================"
}

function trigger_fetch() {
    echo "è¯·è¾“å…¥æ ‡ç­¾ç‚¹ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰ï¼š"
    read tags

    echo "è¯·è¾“å…¥å¼€å§‹æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:mm:ssï¼‰ï¼š"
    read start_time

    echo "è¯·è¾“å…¥ç»“æŸæ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:mm:ssï¼‰ï¼š"
    read end_time

    IFS=',' read -ra TAG_ARRAY <<< "$tags"
    TAG_JSON=$(printf ',"%s"' "${TAG_ARRAY[@]}")
    TAG_JSON="[${TAG_JSON:1}]"

    curl -X POST http://localhost:8080/database-snapshot-manual \
      -H "Content-Type: application/json" \
      -d "{
        \"tagNames\": $TAG_JSON,
        \"count\": 100,
        \"startTime\": \"${start_time}.000\",
        \"endTime\": \"${end_time}.000\"
      }"
}

while true; do
    show_menu
    read -p "è¯·é€‰æ‹©æ“ä½œ: " choice

    case $choice in
        1) ./start.sh ;;
        2) pkill -f "quarkus:dev" ;;
        3) tail -f logs/application.log ;;
        4) trigger_fetch ;;
        5) curl -s http://localhost:8080/database-snapshot-manual | jq '.' ;;
        6) curl -s http://localhost:8080/q/health | jq '.' ;;
        7) open http://localhost:8080/q/swagger-ui ;;
        0) exit 0 ;;
        *) echo "æ— æ•ˆé€‰æ‹©" ;;
    esac

    echo ""
    read -p "æŒ‰Enterç»§ç»­..."
done
```

---

## ğŸ“Š ç›‘æ§å’Œåˆ†æç¤ºä¾‹

### ç¤ºä¾‹17: å®æ—¶ç›‘æ§è„šæœ¬

```bash
#!/bin/bash

echo "æ•°æ®åº“å¿«ç…§è·å–ç³»ç»Ÿ - å®æ—¶ç›‘æ§"
echo "======================================"

while true; do
    clear
    echo "æ—¶é—´: $(date)"
    echo "======================================"

    # å¥åº·çŠ¶æ€
    echo "å¥åº·çŠ¶æ€:"
    curl -s http://localhost:8080/q/health | jq -r '.status'
    echo ""

    # æœ€è¿‘çš„å·¥ä½œæµ
    echo "æœ€è¿‘çš„å·¥ä½œæµæ‰§è¡Œ:"
    curl -s http://localhost:8080/database-snapshot-manual | \
      jq -r '.[] | "\(.id): \(.workflowdata.status)"' | head -5

    echo "======================================"
    sleep 5
done
```

---

**æ›´å¤šç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼Œè¯·å‚è€ƒ [README.md](README.md)**
